{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nayak, Anil Kumar\n",
    "# 1001-396-015\n",
    "# 2017-03-25\n",
    "# Project_Report_02_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils\n",
    "import numpy as np\n",
    "from imutils.object_detection import non_max_suppression\n",
    "import cv2.ml as svmlib\n",
    "import cv2\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_video_details(train_base_path):\n",
    "    training_class_path = os.listdir(train_base_path)\n",
    "    traing_data_details = []\n",
    "\n",
    "    for class_path in training_class_path:\n",
    "        details = {}\n",
    "        details['class'] = class_path\n",
    "        details['path'] = train_base_path +\"\\\\\"+ class_path\n",
    "        videos_names = os.listdir(train_base_path +\"\\\\\"+ class_path)\n",
    "        details['videos']  = videos_names\n",
    "        traing_data_details.append(details)\n",
    "\n",
    "    return traing_data_details\n",
    "\n",
    "def prepare_data_for_clustering(training_data,training_class,flag,number_of_features):\n",
    "    training_matrix_data = []\n",
    "    training_matrix_class = []\n",
    "\n",
    "    for index in range(0,np.shape(training_data)[0]):\n",
    "        frames = training_data[index]\n",
    "\n",
    "        if flag == 1:\n",
    "            if np.size(frames) > 18:\n",
    "                row = []\n",
    "                for frame in frames:\n",
    "                    frame_array = np.asarray(frame,dtype=int)\n",
    "                    row = np.append(row,frame_array)\n",
    "                feature = np.size(row)\n",
    "\n",
    "                if number_of_features < feature:\n",
    "                    number_of_features = feature\n",
    "                training_matrix_data.append(row)\n",
    "                training_matrix_class.append(training_class[index])\n",
    "        else:\n",
    "            row = []\n",
    "            for frame in frames:\n",
    "                frame_array = np.asarray(frame, dtype=int)\n",
    "                row = np.append(row, frame_array)\n",
    "\n",
    "            training_matrix_data.append(row)\n",
    "            training_matrix_class.append(training_class[index])\n",
    "\n",
    "    matrix = np.zeros((np.shape(training_matrix_data)[0],number_of_features), dtype=np.float32)\n",
    "    for index in range(0,np.shape(training_matrix_data)[0]):\n",
    "        row = training_matrix_data[index]\n",
    "        count = 0\n",
    "        for element in row:\n",
    "            matrix[index][count] = element\n",
    "            count = count + 1\n",
    "        # row_size = np.size(row)\n",
    "        # required_zeros = number_of_features-row_size\n",
    "        # if required_zeros > 0:\n",
    "\n",
    "    return matrix,np.asarray(training_matrix_class),number_of_features\n",
    "\n",
    "\n",
    "def get_class_label(class_dtl,class_definition):\n",
    "    return class_definition[class_dtl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Feature in Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_detection(traing_data_details,class_definition):\n",
    "    training_data = []\n",
    "    training_class = []\n",
    "    for training in traing_data_details:\n",
    "        class_dtl = training['class']\n",
    "        video_path = training['path']\n",
    "        videos = training['videos']\n",
    "\n",
    "        for video in videos:\n",
    "\n",
    "            # cv2.namedWindow(\"preview\")\n",
    "            cam = cv2.VideoCapture(video_path + \"\\\\\" + video)\n",
    "            hog = cv2.HOGDescriptor()\n",
    "            hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "            try:\n",
    "                all_frame_in_one_video = []\n",
    "                while True:\n",
    "                    _, image1 = cam.read()\n",
    "                    image = imutils.resize(image1, width=min(400, image1.shape[1]))\n",
    "                    orig = image.copy()\n",
    "\n",
    "                    # detect people in the image\n",
    "                    (rects, weights) = hog.detectMultiScale(image, winStride=(2, 2), padding=(8, 8), scale=1.05)\n",
    "                    # print(rects)\n",
    "                    # draw the original bounding boxes\n",
    "                    for (x, y, w, h) in rects:\n",
    "                        y = y + 10\n",
    "                        pad_w, pad_h = int(0.2 * w), int(0.2 * h)\n",
    "\n",
    "                        top_x = x + pad_w + int(w / 2)\n",
    "                        top_y = y + pad_h\n",
    "\n",
    "                        mid_x = x + pad_w + int(w / 2)\n",
    "                        mid_y = y + pad_h + int((h - pad_h) / 2)\n",
    "\n",
    "                        bottom_x = x + pad_w + int(w / 2)\n",
    "                        bottom_y = y + pad_h + h - pad_h\n",
    "\n",
    "                        one_frame_detail = [top_x, top_y, mid_x, mid_y, bottom_x, bottom_y]\n",
    "                        all_frame_in_one_video.append(one_frame_detail)\n",
    "\n",
    "                        cv2.rectangle(orig, (x + pad_w, y + pad_h), (x + w - pad_w, y + h - pad_h), (0, 255, 0), 1)\n",
    "\n",
    "                    # apply non-maxima suppression to the bounding boxes using a\n",
    "                    # fairly large overlap threshold to try to maintain overlapping\n",
    "                    # boxes that are still people\n",
    "                    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n",
    "                    pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n",
    "\n",
    "                    # draw the final bounding boxes\n",
    "                    # for (xA, yA, xB, yB) in pick:\n",
    "                    #    cv2.rectangle(image, (xA, yA), (xB, yB), (0, 255, 0), 2)\n",
    "\n",
    "                    # show some information on the number of bounding boxes\n",
    "                    # filename = imagePath[imagePath.rfind(\"/\") + 1:]\n",
    "                    # print(\"[INFO] {}: {} original boxes, {} after suppression\".format(\n",
    "                    #   filename, len(rects), len(pick)))\n",
    "\n",
    "\n",
    "                    # cv2.imshow(\"preview\", orig)\n",
    "\n",
    "                    #key = cv2.waitKey(10)\n",
    "                    #if key == 27:\n",
    "                    #    cv2.destroyWindow(\"preview\")\n",
    "                    #    break\n",
    "\n",
    "            except:\n",
    "                # print(np.size(all_frame_in_one_video))\n",
    "                if np.size(all_frame_in_one_video) != 0:\n",
    "                    training_data.append(all_frame_in_one_video)\n",
    "                    training_class.append(get_class_label(class_dtl, class_definition))\n",
    "\n",
    "            cam.release()\n",
    "            # cv2.destroyWindow(\"preview\")\n",
    "\n",
    "    return training_data,training_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_svm(trainData,responses):\n",
    "    svm_params = dict( kernel_type = svmlib.SVM_LINEAR, svm_type=svmlib.SVM_C_SVC, C=2.67, gamma=5.383)\n",
    "\n",
    "    svm = cv2.ml.SVM_create()\n",
    "    svm.train(trainData,cv2.ml.ROW_SAMPLE,responses )#p=svm_paramaramss\n",
    "    svm.save('svm_data.dat')\n",
    "    return svm\n",
    "\n",
    "def predict(testData):\n",
    "    svm = cv2.ml.SVM_create()\n",
    "    svm.load('svm_data.dat')\n",
    "    result = svm.predict(testData)\n",
    "    return result\n",
    "\n",
    "def predict_multiple_data(testData,svm):\n",
    "    results = svm.predict(testData)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_data_all(test_base_path,class_definition,number_of_features,svm):\n",
    "    testing_data_details = read_video_details(test_base_path)\n",
    "\n",
    "    # Find Feature in Test Data\n",
    "    testing_data, testing_class = feature_detection(testing_data_details, class_definition)\n",
    "\n",
    "    # Prepare Testing Data for prediction\n",
    "    flag = 0\n",
    "    testing_matrix_data, testing_matrix_class,number_of_features = prepare_data_for_clustering(testing_data, testing_class, flag,number_of_features)\n",
    "\n",
    "    # Predict Class for Testing Data\n",
    "    result = predict_multiple_data(testing_matrix_data,svm)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.]]\n"
     ]
    }
   ],
   "source": [
    "train_base_path = \"train\"\n",
    "class_definition = {'jump': 1, 'run': 2,'pjump': 3,'skip': 4,'walk': 5}\n",
    "\n",
    "#Fetching Videos for Training\n",
    "# train_base_path is outer folder where all the folder resides\n",
    "# Ex\n",
    "# train\n",
    "#   -walk <forlder name is for class label for video>\n",
    "#       -Videos <training videos>\n",
    "#   -jump\n",
    "#       -Videos\n",
    "training_data_details = read_video_details(train_base_path)\n",
    "\n",
    "#Detect Human and their skeleton and find the feature for classification\n",
    "training_data,training_class = feature_detection(training_data_details,class_definition)\n",
    "\n",
    "#Preparing data for classification\n",
    "flag = 1\n",
    "number_of_features = 0\n",
    "training_matrix_data,training_matrix_class,number_of_features = prepare_data_for_clustering(training_data,training_class,flag,number_of_features)\n",
    "training_matrix_data = np.float32(training_matrix_data)\n",
    "\n",
    "#Training Multiple Class SVM Classifier\n",
    "svm = train_svm(training_matrix_data,training_matrix_class)\n",
    "\n",
    "\n",
    "#Testing Data\n",
    "# test\n",
    "#   -walk <forlder name is for class label for video>\n",
    "#       -Videos <testing videos>\n",
    "#   -jump\n",
    "#       -Videos\n",
    "test_base_path = \"test\"\n",
    "#class_definition = {'jump': 1, 'run': 2,'pjump': 3,'skip': 4,'walk': 5}\n",
    "result = predict_data_all(test_base_path,class_definition,number_of_features,svm)\n",
    "\n",
    "print(result[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
